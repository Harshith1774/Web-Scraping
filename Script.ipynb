{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapegraph_py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jkk0OMSUgnF",
        "outputId": "5937bcfc-7bec-432a-c514-6e3c4a848813"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapegraph_py\n",
            "  Downloading scrapegraph_py-1.10.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.11/dist-packages (from scrapegraph_py) (3.11.11)\n",
            "Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from scrapegraph_py) (4.12.3)\n",
            "Requirement already satisfied: pydantic>=2.10.2 in /usr/local/lib/python3.11/dist-packages (from scrapegraph_py) (2.10.5)\n",
            "Collecting python-dotenv>=1.0.1 (from scrapegraph_py)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from scrapegraph_py) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->scrapegraph_py) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->scrapegraph_py) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->scrapegraph_py) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->scrapegraph_py) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->scrapegraph_py) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->scrapegraph_py) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->scrapegraph_py) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.3->scrapegraph_py) (2.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.2->scrapegraph_py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.2->scrapegraph_py) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.2->scrapegraph_py) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->scrapegraph_py) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->scrapegraph_py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->scrapegraph_py) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->scrapegraph_py) (2024.12.14)\n",
            "Downloading scrapegraph_py-1.10.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, scrapegraph_py\n",
            "Successfully installed python-dotenv-1.0.1 scrapegraph_py-1.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zTspH4qUTMKS"
      },
      "outputs": [],
      "source": [
        "from scrapegraph_py import Client\n",
        "\n",
        "client = Client(api_key=\"sgai-f90c10f0-7453-45fa-814a-58d4cce1dcbb\")\n",
        "\n",
        "response = client.smartscraper(\n",
        "    website_url=\"https://www.reed.co.uk/jobs?sortBy=displayDate&pageno=2&dateCreatedOffSet=lastweek\",\n",
        "    user_prompt=\"Extract all job listings on this page with the following details: Job title, Company name, Location, Job type, Posting date, Salary (if available, display as Not specified if missing), Job summary/description, Apply link (the URL associated with the Apply button or job title link), Provide the results as a structured JSON array.Also make sure to collect job postings from first 3 pages. I want the output to be in csv format in this specific order Job title, Company name, posted on, Salary, Job type, Location, description and apply link\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=response['result']['job_listings']"
      ],
      "metadata": {
        "id": "tKK3OPLRThVw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Specify the output CSV file\n",
        "csv_file = \"output.csv\"\n",
        "\n",
        "# Convert JSON array to CSV\n",
        "if data:  # Check if the JSON array is not empty\n",
        "    # Extract the column headers from the keys of the first dictionary\n",
        "    headers = data[0].keys()\n",
        "\n",
        "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=headers)\n",
        "\n",
        "        # Write the headers\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write the rows\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"JSON data has been saved to {csv_file}\")\n",
        "else:\n",
        "    print(\"No data available to convert.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhep05EtWEQF",
        "outputId": "62422038-722d-422a-fac7-638502e6f061"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON data has been saved to output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GjOgGMSdOXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}